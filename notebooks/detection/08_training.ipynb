{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99331944-50bb-420b-9cd4-2839e074a6ac",
   "metadata": {},
   "source": [
    "### DETR model training ###\n",
    "Build the model training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a053e7e-2f38-4aac-8416-631678286066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 12040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/pytorch/pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project version: v0.0.1\n",
      "Python version:  3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Hugging Face Library\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import computervision\n",
    "from computervision.imageproc import is_image\n",
    "from computervision.datasets import DTRdataset, get_gpu_info\n",
    "from computervision.transformations import AugmentationTransform\n",
    "from computervision.mapeval import MAPEvaluator\n",
    "\n",
    "print(f'Project version: {computervision.__version__}')\n",
    "print(f'Python version:  {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39790571-4821-4972-8005-45bdd7dfc7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Number of GPUs found:  1\n",
      "Date: 250925\n"
     ]
    }
   ],
   "source": [
    "# Set training device\n",
    "device, device_str = get_gpu_info()\n",
    "# Save the date in a string\n",
    "date_str = datetime.date.today().strftime('%y%m%d')\n",
    "print(f'Date: {date_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a3f138-d71a-4b73-b91c-db6342c80828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtdetr_250923_01\n"
     ]
    }
   ],
   "source": [
    "# Directories and files\n",
    "data_dir = os.path.join(os.environ.get('HOME'), 'data')\n",
    "train_image_dir = os.path.join(data_dir, 'dentex', 'cropped')\n",
    "val_image_dir = os.path.join(train_image_dir, 'test')\n",
    "model_dir = os.path.join(data_dir, 'model')\n",
    "train_annotation_file_name = 'train_quadrant_enumeration_dset.parquet'\n",
    "train_annotation_file = os.path.join(train_image_dir, train_annotation_file_name)\n",
    "val_annotation_file_name = 'train_quadrant_enumeration_test_set.parquet'\n",
    "val_annotation_file = os.path.join(val_image_dir, val_annotation_file_name)\n",
    "\n",
    "# Column names for the annotation files\n",
    "label_col = 'label'\n",
    "label_name_col = 'ada'\n",
    "file_name_col = 'file_name'\n",
    "bbox_col = 'bbox'\n",
    "dset_col = 'dset'\n",
    "\n",
    "# Training and model parameters\n",
    "model_version = 1\n",
    "im_size = 640\n",
    "model_name = f'rtdetr_{date_str}_{str(model_version).zfill(2)}'\n",
    "print(model_name)\n",
    "\n",
    "# Important information about the model that we want to save\n",
    "model_info = {'model_version': model_version,\n",
    "              'project_version': computervision.__version__,\n",
    "              'model_name': model_name,\n",
    "              'model_dir': model_dir,\n",
    "              'train_image_dir': train_image_dir,\n",
    "              'val_image_dir': val_image_dir,\n",
    "              'model_dir': model_dir,\n",
    "              'image_width': im_size,\n",
    "              'image_height': im_size,\n",
    "              'hf_checkpoint': 'PekingU/rtdetr_v2_r101vd',\n",
    "              'train_transform': 'train_1',\n",
    "              'val_transform': 'val_1'}\n",
    "\n",
    "# Specific arguments for the Trainer.\n",
    "# See: https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer\n",
    "training_args = {'output_dir': os.path.join(model_dir, model_name),\n",
    "                 'num_train_epochs': 20,\n",
    "                 'max_grad_norm': 0.1,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'warmup_steps': 300,\n",
    "                 'per_device_train_batch_size': 4,\n",
    "                 'dataloader_num_workers': 2,\n",
    "                 'metric_for_best_model': 'eval_map',\n",
    "                 'greater_is_better': True,\n",
    "                 'load_best_model_at_end': True,\n",
    "                 'eval_strategy': 'epoch',\n",
    "                 'save_strategy': 'epoch',\n",
    "                 'save_total_limit': 2,\n",
    "                 'remove_unused_columns': False,\n",
    "                 'eval_do_concat_batches': False}\n",
    "\n",
    "# We want to maintain the aspect ratio of the images\n",
    "# So, we resize the image first and then pad it\n",
    "processor_params = {'do_resize': True,\n",
    "                    'size': {'max_height': im_size,\n",
    "                             'max_width': im_size},\n",
    "                    'do_pad': True,\n",
    "                    'pad_size': {'height': im_size,\n",
    "                                 'width': im_size}}\n",
    "\n",
    "model_dict = {'model_info': model_info, \n",
    "              'training_args': training_args, \n",
    "              'processor_params': processor_params}\n",
    "\n",
    "# Dumpt the model parameters to a file\n",
    "json_file = os.path.join(model_dir, f'{model_name}.json')\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(model_dict, f, indent=4) # indent for pretty-printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8d88d-f48d-43d4-ac5f-46054e4a2304",
   "metadata": {},
   "source": [
    "### Verify the image data ###\n",
    "Make sure that the directories and annotation files are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "564b09d8-d515-42e2-81d4-02fd5b91fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in training data:         2391\n",
      "Files checked in training data:  2391\n",
      "Annotations in training data:    34166\n",
      "\n",
      "Images in validation data:       32\n",
      "Files checked in val data:       32\n",
      "Annotations in validation data:  372\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet(train_annotation_file)\n",
    "train_df = train_df.loc[train_df[dset_col] == 'train']\n",
    "val_df = pd.read_parquet(val_annotation_file)\n",
    "# Filter the validation images and quadrants and take only the first augmentation\n",
    "val_df = val_df.loc[\n",
    "    (val_df['dset'] == 'val') & \n",
    "    (val_df['quadrants'].isin([14, 23])) & \n",
    "    (val_df['transformation'] == 0)]\n",
    "\n",
    "# Check the images on disk\n",
    "train_file_list = list(train_df[file_name_col].unique())\n",
    "train_checked = np.sum([is_image(os.path.join(train_image_dir, file)) for file in train_file_list])\n",
    "print(f'Images in training data:         {len(train_file_list)}')\n",
    "print(f'Files checked in training data:  {train_checked}')\n",
    "print(f'Annotations in training data:    {train_df.shape[0]}')\n",
    "print()\n",
    "val_file_list = list(val_df[file_name_col].unique())\n",
    "val_checked = np.sum([is_image(os.path.join(val_image_dir, file)) for file in val_file_list])\n",
    "print(f'Images in validation data:       {len(val_file_list)}')\n",
    "print(f'Files checked in val data:       {val_checked}')\n",
    "print(f'Annotations in validation data:  {val_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826627a-eb1c-49da-81cf-97609165b414",
   "metadata": {},
   "source": [
    "### Set up the logger ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb9d0d7-d0e4-41a3-8a5f-5f2d988dab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "log_file_name = f'{model_name}.log'\n",
    "log_file = os.path.join(model_dir, log_file_name)\n",
    "dtfmt = '%y%m%d-%H:%M'\n",
    "logfmt = '%(asctime)s-%(name)s-%(levelname)s-%(message)s'\n",
    "\n",
    "logging.basicConfig(filename=log_file,\n",
    "                    filemode='w',\n",
    "                    level=logging.INFO,\n",
    "                    format=logfmt,\n",
    "                    datefmt=dtfmt,\n",
    "                    force=True)\n",
    "\n",
    "logger = logging.getLogger(name=__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a324b7-8ac1-4c66-ac22-9e64b2c8bf54",
   "metadata": {},
   "source": [
    "### Datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f843df12-6a86-41b6-8f10-fa07d9ab7f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbox</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_base_name</th>\n",
       "      <th>quadrants</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>pos</th>\n",
       "      <th>fdi</th>\n",
       "      <th>ada</th>\n",
       "      <th>dset</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[666, 102, 103, 376]</td>\n",
       "      <td>[[757, 478, 769, 102, 678, 113, 666, 469]]</td>\n",
       "      <td>494</td>\n",
       "      <td>1473</td>\n",
       "      <td>train_0_12.png</td>\n",
       "      <td>train_0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[593, 107, 85, 377]</td>\n",
       "      <td>[[666, 484, 678, 110, 607, 107, 604, 299, 619,...</td>\n",
       "      <td>494</td>\n",
       "      <td>1473</td>\n",
       "      <td>train_0_12.png</td>\n",
       "      <td>train_0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bbox                                       segmentation  \\\n",
       "0  [666, 102, 103, 376]         [[757, 478, 769, 102, 678, 113, 666, 469]]   \n",
       "1   [593, 107, 85, 377]  [[666, 484, 678, 110, 607, 107, 604, 299, 619,...   \n",
       "\n",
       "   height  width       file_name file_base_name  quadrants  quadrant pos  fdi  \\\n",
       "0     494   1473  train_0_12.png        train_0         12         1   1   11   \n",
       "1     494   1473  train_0_12.png        train_0         12         1   2   12   \n",
       "\n",
       "   ada   dset  label  \n",
       "0    8  train      7  \n",
       "1    7  train      6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbox</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>ada</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_base_name</th>\n",
       "      <th>quadrants</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>transformation</th>\n",
       "      <th>transformation_name</th>\n",
       "      <th>dset</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[459, 0, 11, 323]</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>val_train_143_14_00.png</td>\n",
       "      <td>train_143</td>\n",
       "      <td>14</td>\n",
       "      <td>471</td>\n",
       "      <td>516</td>\n",
       "      <td>0</td>\n",
       "      <td>test_set</td>\n",
       "      <td>val</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[399, 8, 70, 294]</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>val_train_143_14_00.png</td>\n",
       "      <td>train_143</td>\n",
       "      <td>14</td>\n",
       "      <td>471</td>\n",
       "      <td>516</td>\n",
       "      <td>0</td>\n",
       "      <td>test_set</td>\n",
       "      <td>val</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bbox  quadrant  ada                file_name file_base_name  \\\n",
       "125  [459, 0, 11, 323]         1    8  val_train_143_14_00.png      train_143   \n",
       "126  [399, 8, 70, 294]         1    7  val_train_143_14_00.png      train_143   \n",
       "\n",
       "     quadrants  height  width  transformation transformation_name dset  label  \n",
       "125         14     471    516               0            test_set  val      7  \n",
       "126         14     471    516               0            test_set  val      6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the label ids (tooth position, but starting from 0)\n",
    "# The model needs label ids, not labels. So we need to add a label id column\n",
    "label_name_list = sorted(list(train_df[label_name_col].unique()))\n",
    "id2label = dict(zip(range(len(label_name_list)), label_name_list))\n",
    "id2label = {int(label_id): str(label_name) for label_id, label_name in id2label.items()}\n",
    "label2id = {str(label_name): int(label_id) for label_id, label_name in id2label.items()}\n",
    "\n",
    "train_df = train_df.assign(label=train_df[label_name_col].apply(lambda name: label2id.get(str(name))))\n",
    "val_df = val_df.assign(label=val_df[label_name_col].apply(lambda name: label2id.get(str(name))))\n",
    "display(train_df.head(2))\n",
    "display(val_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b864f1a-3f93-4cbf-bee7-214ab58d3fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomCropFromBorders(p=1.0, crop_bottom=0.5, crop_left=0.3, crop_right=0.3, crop_top=0.5)\n",
      "CenterCrop(p=1.0, border_mode=0, fill=0.0, fill_mask=0.0, height=640, pad_if_needed=True, pad_position='center', width=640)\n",
      "Affine(p=0.5, balanced_scale=False, border_mode=0, fill=0.0, fill_mask=0.0, fit_output=False, interpolation=1, keep_ratio=False, mask_interpolation=0, rotate=(1.0, 1.0), rotate_method='largest_box', scale={'x': (0.8, 1.2), 'y': (0.8, 1.2)}, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)})\n",
      "RandomBrightnessContrast(p=0.5, brightness_by_max=True, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), ensure_safe_range=False)\n",
      "Sharpen(p=0.5, alpha=(0.2, 0.5), kernel_size=5, lightness=(0.5, 1.0), method='kernel', sigma=1.0)\n",
      "CLAHE(p=0.5, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "CenterCrop(p=1.0, border_mode=0, fill=0.0, fill_mask=0.0, height=640, pad_if_needed=True, pad_position='center', width=640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r101vd and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([33, 256]) in the model instantiated\n",
      "- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([32, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAPEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     16\u001b[39m model = RTDetrV2ForObjectDetection.\\\n\u001b[32m     17\u001b[39m     from_pretrained(model_checkpoint,\n\u001b[32m     18\u001b[39m                     id2label=id2label,\n\u001b[32m     19\u001b[39m                     label2id=label2id,\n\u001b[32m     20\u001b[39m                     anchor_image_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     21\u001b[39m                     ignore_mismatched_sizes=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Set the evaluation metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m eval_compute_metrics_fn = \u001b[43mMAPEvaluator\u001b[49m(image_processor=image_processor, threshold=\u001b[32m0.01\u001b[39m, id2label=id2label)\n\u001b[32m     25\u001b[39m training_args = TrainingArguments(**training_args)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Create the data sets\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'MAPEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the augmentation transforms\n",
    "aug = AugmentationTransform(image_width=model_info.get('image_width'), \n",
    "                            image_height=model_info.get('image_height'))\n",
    "train_transform = aug.get_transforms(name=model_info.get('train_transform'))\n",
    "val_transform = aug.get_transforms(name=model_info.get('val_transform'))\n",
    "print(*train_transform, sep='\\n')\n",
    "print()\n",
    "print(*val_transform, sep='\\n')\n",
    "\n",
    "# Load the image processor\n",
    "model_checkpoint = model_info.get('hf_checkpoint')\n",
    "image_processor = RTDetrImageProcessor.\\\n",
    "    from_pretrained(model_checkpoint, **processor_params)\n",
    "\n",
    "# Load model from checkpoint\n",
    "model = RTDetrV2ForObjectDetection.\\\n",
    "    from_pretrained(model_checkpoint,\n",
    "                    id2label=id2label,\n",
    "                    label2id=label2id,\n",
    "                    anchor_image_size=None,\n",
    "                    ignore_mismatched_sizes=True)\n",
    "\n",
    "# Set the evaluation metrics\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=image_processor, threshold=0.01, id2label=id2label)\n",
    "training_args = TrainingArguments(**training_args)\n",
    "\n",
    "# Create the data sets\n",
    "train_dataset = DTRdataset(data=train_df,\n",
    "                           image_processor=image_processor,\n",
    "                           image_dir=train_image_dir,\n",
    "                           file_name_col=file_name_col,\n",
    "                           label_id_col='label',\n",
    "                           bbox_col=bbox_col,\n",
    "                           transforms=train_transform)\n",
    "\n",
    "val_dataset = DTRdataset(data=train_df,\n",
    "                         image_processor=image_processor,\n",
    "                         image_dir=train_image_dir,\n",
    "                         file_name_col=file_name_col,\n",
    "                         label_id_col='label',\n",
    "                         bbox_col=bbox_col,\n",
    "                         transforms=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ce508-07ec-4804-ae03-fa6d6ddb4ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
