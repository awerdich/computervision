{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99331944-50bb-420b-9cd4-2839e074a6ac",
   "metadata": {},
   "source": [
    "### DETR model training ###\n",
    "Build the model training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a053e7e-2f38-4aac-8416-631678286066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Project version: v0.0.2\n",
      "Python version:  3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Hugging Face Library\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import computervision\n",
    "from computervision.imageproc import is_image\n",
    "from computervision.datasets import DETRdataset, get_gpu_info\n",
    "from computervision.transformations import AugmentationTransform\n",
    "from computervision.mapeval import MAPEvaluator\n",
    "\n",
    "print(f'Project version: {computervision.__version__}')\n",
    "print(f'Python version:  {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39790571-4821-4972-8005-45bdd7dfc7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs found:  1\n",
      "Current device ID: 0\n",
      "GPU device name:   NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "PyTorch version:   2.8.0a0+34c6371d24.nv25.08\n",
      "CUDA version:      13.0\n",
      "CUDNN version:     91200\n",
      "Device for model training/inference: cuda:0\n",
      "Date: 251005\n"
     ]
    }
   ],
   "source": [
    "# Set training device\n",
    "device, device_str = get_gpu_info()\n",
    "# Save the date in a string\n",
    "date_str = datetime.date.today().strftime('%y%m%d')\n",
    "print(f'Date: {date_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4a431c-9514-437b-a90d-4a8f4ea5adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/data_model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('DATA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f213f4-0115-473e-af8f-14bd9278739d",
   "metadata": {},
   "source": [
    "### Data files and directories ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fccce9f-7796-48a2-b538-da36af29a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: rtdetr_roboflow_251005_01\n"
     ]
    }
   ],
   "source": [
    "model_version = 1\n",
    "date_str = datetime.date.today().strftime('%y%m%d')\n",
    "dataset = 'roboflow'\n",
    "model_name = f'rtdetr_{dataset}_{date_str}_{str(model_version).zfill(2)}'\n",
    "print(f'Model name: {model_name}')\n",
    "data_dir = os.environ.get('DATA')\n",
    "if data_dir is None:\n",
    "    raise ValueError(\"DATA environment variable must be set\")\n",
    "\n",
    "model_dir = os.path.join(data_dir, 'model')\n",
    "model_name_dir = os.path.join(model_dir, model_name)\n",
    "Path(model_name_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_image_dir = os.path.join(data_dir, 'dataset_object_roboflow_240930')\n",
    "train_annotation_file_name = 'roboflow_dset.parquet'\n",
    "train_annotation_file = os.path.join(train_image_dir, train_annotation_file_name)\n",
    "\n",
    "val_image_dir = train_image_dir\n",
    "val_annotation_file_name = train_annotation_file_name\n",
    "val_annotation_file = os.path.join(val_image_dir, val_annotation_file_name)\n",
    "\n",
    "# Column names for the annotation files\n",
    "tooth_pos_col = 'pos'\n",
    "file_name_col = 'multi_file'\n",
    "bbox_col = 'bbox'\n",
    "dset_col = 'dset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9e75c-c846-468d-8baf-f65c3f21b6c6",
   "metadata": {},
   "source": [
    "### Model and training parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70d92c8c-dd52-48d6-8e82-04e5e0d16a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device for model training/inference: cuda:0\n",
      "training_checkpoint: /app/data_model/model/rtdetr_251004_01/checkpoint-7500\n"
     ]
    }
   ],
   "source": [
    "device_number = 0\n",
    "device, device_str = get_gpu_info(device_number=device_number)\n",
    "\n",
    "# Image transformations for training and validation\n",
    "im_width, im_height = 640, 640\n",
    "# Augmentations\n",
    "train_transform_name = 'train_roboflow'\n",
    "val_transform_name = 'val'\n",
    "aug = AugmentationTransform(im_width=im_width, im_height=im_height)\n",
    "train_transforms = aug.get_transforms(name=train_transform_name)\n",
    "val_transforms = aug.get_transforms(name=val_transform_name)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint_model_name = 'rtdetr_251004_01'\n",
    "checkpoint = 'checkpoint-7500'\n",
    "checkpoint_dir = os.path.join(model_dir, \n",
    "                              checkpoint_model_name, \n",
    "                              checkpoint)\n",
    "\n",
    "print(f'training_checkpoint: {checkpoint_dir}')\n",
    "\n",
    "# Important information about the model that we want to save\n",
    "model_info = {'model_version': model_version,\n",
    "              'device_number': device_number,\n",
    "              'dataset': dataset,\n",
    "              'project_version': computervision.__version__,\n",
    "              'model_name': model_name,\n",
    "              'train_image_dir': train_image_dir,\n",
    "              'val_image_dir': val_image_dir,\n",
    "              'im_width': im_width,\n",
    "              'im_height': im_height,\n",
    "              'hf_checkpoint': 'PekingU/rtdetr_v2_r101vd',\n",
    "              'training_checkpoint': checkpoint_dir,\n",
    "              'train_quadrants': np.nan,\n",
    "              'val_quadrants': np.nan,\n",
    "              'train_transform_name': train_transform_name,\n",
    "              'val_transform_name': val_transform_name,\n",
    "              'val_score_threshold': 0.02}\n",
    "\n",
    "# Specific arguments for the Trainer. 48\n",
    "# See: https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer\n",
    "training_args = {'output_dir': model_name_dir,\n",
    "                 'num_train_epochs': 2,\n",
    "                 'max_grad_norm': 0.1,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'warmup_steps': 300,\n",
    "                 'per_device_train_batch_size': 4,\n",
    "                 'dataloader_num_workers': 8,\n",
    "                 'metric_for_best_model': 'eval_map',\n",
    "                 'greater_is_better': True,\n",
    "                 'load_best_model_at_end': True,\n",
    "                 'eval_strategy': 'epoch',\n",
    "                 'save_strategy': 'epoch',\n",
    "                 'save_total_limit': 5,\n",
    "                 'remove_unused_columns': False,\n",
    "                 'eval_do_concat_batches': False}\n",
    "\n",
    "# We want to maintain the aspect ratio of the images\n",
    "# So, we resize the image first and then pad it\n",
    "processor_params = {'do_resize': True,\n",
    "                    'size': {'max_height': im_height,\n",
    "                             'max_width': im_width},\n",
    "                    'do_pad': True,\n",
    "                    'pad_size': {'height': im_height,\n",
    "                                 'width': im_width}}\n",
    "\n",
    "# Bounding box format for the annotations\n",
    "bbox_format = {'format': 'coco',\n",
    "               'label_fields': ['tooth_position'],\n",
    "               'clip': True,\n",
    "               'min_area': 10000}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16769bed-4f8e-4835-9314-2922cf2bfb9d",
   "metadata": {},
   "source": [
    "### Verify image data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e532baf2-4924-4f4e-9ede-25b7252cfb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in training data:         916\n",
      "Files checked in training data:  916\n",
      "Annotations in training data:    8114\n",
      "\n",
      "Images in validation data:       25\n",
      "Files checked in val data:       25\n",
      "Annotations in validation data:  214\n"
     ]
    }
   ],
   "source": [
    "#%% Verify image data\n",
    "train_df = pd.read_parquet(train_annotation_file)\n",
    "train_df = train_df.loc[train_df[dset_col] == 'train'].astype({tooth_pos_col: int})\n",
    "\n",
    "# Filter the validation images and quadrants and take only the first augmentation\n",
    "val_df = pd.read_parquet(val_annotation_file)\n",
    "val_df = val_df.loc[val_df[dset_col] == 'val'].astype({tooth_pos_col: int})\n",
    "\n",
    "# Check the images on disk\n",
    "train_file_list = list(train_df[file_name_col].unique())\n",
    "train_checked = np.sum([is_image(os.path.join(train_image_dir, file)) for file in train_file_list])\n",
    "print(f'Images in training data:         {len(train_file_list)}')\n",
    "print(f'Files checked in training data:  {train_checked}')\n",
    "print(f'Annotations in training data:    {train_df.shape[0]}')\n",
    "\n",
    "print()\n",
    "\n",
    "val_file_list = list(val_df[file_name_col].unique())\n",
    "val_checked = np.sum([is_image(os.path.join(val_image_dir, file)) for file in val_file_list])\n",
    "print(f'Images in validation data:       {len(val_file_list)}')\n",
    "print(f'Files checked in val data:       {val_checked}')\n",
    "print(f'Annotations in validation data:  {val_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d99c6c8b-eb14-46ac-bc12-4c0c45c9afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the label ids (tooth position, but starting from 0)\n",
    "# The model needs label ids, not labels. So we need to add a label id column\n",
    "label_name_list = np.arange(1, 33, 1)\n",
    "id_list = [(n-1) for n in label_name_list]\n",
    "\n",
    "id2label = dict(zip(id_list, label_name_list))\n",
    "id2label = {int(label_id): str(label_name) for label_id, label_name in id2label.items()}\n",
    "label2id = {str(label_name): int(label_id) for label_id, label_name in id2label.items()}\n",
    "\n",
    "train_df = train_df.assign(label=train_df[tooth_pos_col].apply(lambda name: label2id.get(str(name))))\n",
    "val_df = val_df.assign(label=val_df[tooth_pos_col].apply(lambda name: label2id.get(str(name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e319cf-d234-4ce6-9311-24df0a5ff90c",
   "metadata": {},
   "source": [
    "### Logger ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d09219d-09c6-4546-9d1c-898f910a6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create the logger\n",
    "\n",
    "parameters = {'model_info': model_info,\n",
    "              'id2label': id2label,\n",
    "              'training_args': training_args,\n",
    "              'processor_params': processor_params,\n",
    "              'bbox_format': bbox_format}\n",
    "\n",
    "json_file = os.path.join(model_name_dir, f'{model_name}.json')\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(parameters, f, indent=4) # indent for pretty-printing\n",
    "\n",
    "# Set up the logger\n",
    "log_file_name = f'{model_name}.log'\n",
    "log_file = os.path.join(model_name_dir, log_file_name)\n",
    "dtfmt = '%y%m%d-%H:%M'\n",
    "logfmt = '%(asctime)s-%(name)s-%(levelname)s-%(message)s'\n",
    "\n",
    "logging.basicConfig(filename=log_file,\n",
    "                    filemode='w',\n",
    "                    level=logging.INFO,\n",
    "                    format=logfmt,\n",
    "                    datefmt=dtfmt,\n",
    "                    force=True)\n",
    "\n",
    "logger = logging.getLogger(name=__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67a12e-7e59-4503-b324-51e457e31598",
   "metadata": {},
   "source": [
    "### Model and image processor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "586202dc-8bbd-4ebc-a0db-43e936188431",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = model_info.get('training_checkpoint')\n",
    "training_checkpoint = model_info.get('training_checkpoint')\n",
    "processor = RTDetrImageProcessor.\\\n",
    "    from_pretrained(model_checkpoint, **processor_params)\n",
    "\n",
    "# Load model from a pretrained checkpoint\n",
    "model = RTDetrV2ForObjectDetection.\\\n",
    "    from_pretrained(training_checkpoint,\n",
    "                    id2label=id2label,\n",
    "                    label2id=label2id,\n",
    "                    anchor_image_size=None,\n",
    "                    ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9fb53-de6e-4bb6-8d94-42ee48018052",
   "metadata": {},
   "source": [
    "### Datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3575c7d5-ac2a-408d-bf21-517ffdbec8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DETRdataset(data=train_df,\n",
    "                            image_processor=processor,\n",
    "                            image_dir=train_image_dir,\n",
    "                            file_name_col=file_name_col,\n",
    "                            label_id_col='label',\n",
    "                            bbox_col=bbox_col,\n",
    "                            transforms=train_transforms)\n",
    "\n",
    "val_dataset = DETRdataset(data=val_df,\n",
    "                          image_processor=processor,\n",
    "                          image_dir=val_image_dir,\n",
    "                          file_name_col=file_name_col,\n",
    "                          label_id_col='label',\n",
    "                          bbox_col=bbox_col,\n",
    "                          transforms=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3654c2b-7d6e-4642-9912-130ff2505d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Training\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collates a batch of data samples into a single dictionary for model input.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data\n",
    "\n",
    "# Set the evaluation metrics\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=processor,\n",
    "                                       threshold=model_info.get('val_score_threshold'),\n",
    "                                       id2label=id2label)\n",
    "\n",
    "training_arguments = TrainingArguments(**training_args)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_arguments,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=val_dataset,\n",
    "                  processing_class=processor,\n",
    "                  data_collator=collate_fn,\n",
    "                  compute_metrics=eval_compute_metrics_fn)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
