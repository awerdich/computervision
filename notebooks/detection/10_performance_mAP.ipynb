{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b8c7ba-53b2-4b86-86e1-db2ab56e4d61",
   "metadata": {},
   "source": [
    "### Performance Evaluation: AP scores ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c792a7e0-6b93-45a9-bc77-17378d1a7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project version: v0.0.1\n",
      "Python version:  3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import ops\n",
    "\n",
    "# Hugging Face Library\n",
    "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import computervision\n",
    "from computervision.imageproc import is_image, ImageData, clipxywh, xyxy2xywh, xywh2xyxy\n",
    "from computervision.datasets import DETRdataset, get_gpu_info\n",
    "from computervision.transformations import AugmentationTransform\n",
    "from computervision.performance import DetectionMetrics\n",
    "from computervision.inference import DETR\n",
    "\n",
    "print(f'Project version: {computervision.__version__}')\n",
    "print(f'Python version:  {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363ffa48-4716-4e20-a169-09cc97360393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs found:  1\n",
      "Current device ID: 0\n",
      "GPU device name:   NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "PyTorch version:   2.8.0a0+34c6371d24.nv25.08\n",
      "CUDA version:      13.0\n",
      "CUDNN version:     91200\n",
      "Device for model training/inference: cuda:0\n",
      "Current device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device, device_str = get_gpu_info()\n",
    "print(f'Current device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74c6673-8888-41f4-b18f-586560a797af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot an image with the bounding boxes\n",
    "def plot_boxes(image, box_list, ax, label_list=None, color=None, cmap='grey', offset_xy=(0, 0)):\n",
    "    offset_xy = (10 + offset_xy[0], 100 + offset_xy[1])\n",
    "    # Take a ratio that looks good\n",
    "    offset = (image.shape[1]*offset_xy[0]/2500,\n",
    "              image.shape[0]*offset_xy[1]/1250)\n",
    "    if color is None:\n",
    "        # If no color is provided, color each box in a different color\n",
    "        color_list = list(plt.cm.rainbow(np.linspace(0, 1, len(box_list))))\n",
    "    else:\n",
    "        color_list = [color]*len(box_list)\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "    ax.imshow(image, cmap=cmap)\n",
    "    # Loop over the bounding boxes\n",
    "    for b, box in enumerate(box_list):\n",
    "        rect = Rectangle(xy=(box[0], box[1]),\n",
    "                         width=box[2],\n",
    "                         height=box[3],\n",
    "                         linewidth=1.5,\n",
    "                         edgecolor=color_list[b],\n",
    "                         facecolor='none',\n",
    "                         alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        if label_list is not None:\n",
    "            ax.text(x=box[0]+offset[0], y=box[1]+offset[1], s=label_list[b], color=color_list[b], fontsize=8)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62805bf-1fe0-4ce1-8e94-b0de0c6e2df7",
   "metadata": {},
   "source": [
    "### Test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9347d-372e-425c-9ca8-ac3a0b6e412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSDM DATA\n",
    "image_dir = os.path.join(os.environ.get('DATA'), 'dataset_object_240921')\n",
    "df_file_name = 'objectdata_240921_clean_ide.parquet'\n",
    "df_file = os.path.join(image_dir, df_file_name)\n",
    "df = pd.read_parquet(df_file)\n",
    "display(df.head(2))\n",
    "\n",
    "file_col = 'multi_file'\n",
    "pos_col = 'pos'\n",
    "bbox_col = 'bbox'\n",
    "\n",
    "file_name_list = sorted(list(df[file_col].unique()))\n",
    "file_list = [os.path.join(image_dir, file) for file in file_name_list]\n",
    "# Check the test data\n",
    "checked = [is_image(file) for file in file_list]\n",
    "assert np.sum(checked) == len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c808d9d9-362f-4141-8367-5e215a4c8066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in Dentex test set: 80\n",
      "Annotations in Dentex test set:      895\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bbox</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>ada</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_base_name</th>\n",
       "      <th>quadrants</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>transformation</th>\n",
       "      <th>transformation_name</th>\n",
       "      <th>dset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11615</th>\n",
       "      <td>[424, 0, 94, 314]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>test_train_219_14_00.png</td>\n",
       "      <td>train_219</td>\n",
       "      <td>14</td>\n",
       "      <td>519</td>\n",
       "      <td>639</td>\n",
       "      <td>0</td>\n",
       "      <td>test_set</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11616</th>\n",
       "      <td>[311, 0, 160, 283]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>test_train_219_14_00.png</td>\n",
       "      <td>train_219</td>\n",
       "      <td>14</td>\n",
       "      <td>519</td>\n",
       "      <td>639</td>\n",
       "      <td>0</td>\n",
       "      <td>test_set</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bbox  quadrant  ada                 file_name  \\\n",
       "11615   [424, 0, 94, 314]       1.0    6  test_train_219_14_00.png   \n",
       "11616  [311, 0, 160, 283]       1.0    5  test_train_219_14_00.png   \n",
       "\n",
       "      file_base_name  quadrants  height  width  transformation  \\\n",
       "11615      train_219         14     519    639               0   \n",
       "11616      train_219         14     519    639               0   \n",
       "\n",
       "      transformation_name  dset  \n",
       "11615            test_set  test  \n",
       "11616            test_set  test  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dentex testing data\n",
    "image_dir = os.path.join(os.environ.get('DATA'), 'dentex_detection_250928', 'test')\n",
    "df_file_name = 'train_quadrant_enumeration_test_set.parquet'\n",
    "df_file = os.path.join(image_dir, df_file_name)\n",
    "df = pd.read_parquet(df_file)\n",
    "df = df.loc[\n",
    "    (df['dset'] == 'test') & \n",
    "    (df['quadrants'].isin([14, 23])) &\n",
    "    (df['transformation'].isin(range(2)))]\n",
    "\n",
    "print(f'Number of images in Dentex test set: {len(df['file_name'].unique())}')\n",
    "print(f'Annotations in Dentex test set:      {df.shape[0]}')\n",
    "\n",
    "file_col = 'file_name'\n",
    "pos_col = 'ada'\n",
    "bbox_col = 'bbox'\n",
    "\n",
    "df = df.astype({pos_col: int})\n",
    "display(df.head(2))\n",
    "\n",
    "file_name_list = sorted(list(df[file_col].unique()))\n",
    "file_list = [os.path.join(image_dir, file) for file in file_name_list]\n",
    "# Check the test data\n",
    "checked = [is_image(file) for file in file_list]\n",
    "assert np.sum(checked) == len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab55c8-3b17-405a-9c9e-fa2bf5e57fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roboflow test data\n",
    "image_dir = os.path.join(os.environ.get('DATA'), 'dataset_object_roboflow_240930')\n",
    "df_file_name = 'roboflow_240930.parquet'\n",
    "df_file = os.path.join(image_dir, df_file_name)\n",
    "df = pd.read_parquet(df_file)\n",
    "display(df.head(2))\n",
    "# What are the categories in this data set\n",
    "print(list(df['category'].unique()))\n",
    "\n",
    "file_col = 'multi_file'\n",
    "pos_col = 'pos'\n",
    "bbox_col = 'bbox'\n",
    "\n",
    "# Let's get rid of the rows that do not have locations\n",
    "df = df.loc[~df[pos_col].isnull()]\n",
    "\n",
    "file_name_list = sorted(list(df[file_col].unique()))\n",
    "file_list = [os.path.join(image_dir, file) for file in file_name_list]\n",
    "# Check the test data\n",
    "checked = [is_image(file) for file in file_list]\n",
    "assert np.sum(checked) == len(file_list)\n",
    "print(f'Using {len(checked)} images from the Roboflow data set')\n",
    "print(f'Annotations: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de5788-b1f2-4bff-9ae8-572a80acf8f8",
   "metadata": {},
   "source": [
    "### Model and image processor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7868849c-17a8-49ae-a567-f90f628b040e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_info', 'id2label', 'training_args', 'processor_params', 'bbox_format'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs found:  1\n",
      "Current device ID: 0\n",
      "GPU device name:   NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "PyTorch version:   2.8.0a0+34c6371d24.nv25.08\n",
      "CUDA version:      13.0\n",
      "CUDNN version:     91200\n",
      "Device for model training/inference: cuda:0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_version': 2,\n",
       " 'device_number': 2,\n",
       " 'project_version': 'v0.0.1',\n",
       " 'model_name': 'rtdetr_251001_02',\n",
       " 'train_image_dir': '/app/data_model/dentex_detection_250928',\n",
       " 'val_image_dir': '/app/data_model/dentex_detection_250928/test',\n",
       " 'model_dir': '/app/data_model/model',\n",
       " 'im_width': 640,\n",
       " 'im_height': 640,\n",
       " 'hf_checkpoint': 'PekingU/rtdetr_v2_r101vd',\n",
       " 'training_checkpoint': 'PekingU/rtdetr_v2_r101vd',\n",
       " 'train_quadrants': [14, 23],\n",
       " 'val_quadrants': [14, 23],\n",
       " 'train_transform_name': 'train_14_23',\n",
       " 'val_transform_name': 'val'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'rtdetr_251001_02'\n",
    "model_dir = os.path.join(os.environ.get('DATA'), 'model', model_name)\n",
    "model_json_name = f'{model_name}.json'\n",
    "model_json_file = os.path.join(model_dir, model_json_name)\n",
    "\n",
    "checkpoint_name = 'checkpoint-17500'\n",
    "checkpoint = os.path.join(model_dir, checkpoint_name)\n",
    "\n",
    "# Load model parameters\n",
    "with open(model_json_file, mode='r') as fl:\n",
    "    model_parameters = json.load(fl)\n",
    "display(model_parameters.keys())\n",
    "\n",
    "# Load the model\n",
    "dtr = DETR(checkpoint_path=checkpoint)\n",
    "\n",
    "model_info = model_parameters.get('model_info')\n",
    "print()\n",
    "display(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eabf588-e0e2-4260-991d-ac94c98c4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category ID to label conversion\n",
    "id2label = model_parameters.get('id2label')\n",
    "id2label = {int(k): int(v) for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3317cc8-d059-4095-8299-a41e1bf875a5",
   "metadata": {},
   "source": [
    "### Run the test data through the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25e59b0-a44b-454b-83e1-afb8d7aa50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation transform\n",
    "transform = AugmentationTransform(im_width=model_info.get('im_width'), \n",
    "                                  im_height=model_info.get('im_height')).\\\n",
    "    get_transforms(name=model_info.get('val_transform_name'))\n",
    "\n",
    "df = df.assign(label=df[pos_col].apply(lambda ada: label2id.get(ada)))\n",
    "\n",
    "test_dataset = DETRdataset(data=df.copy(), \n",
    "                           image_processor=dtr.processor,\n",
    "                           image_dir=image_dir,\n",
    "                           file_name_col=file_col,\n",
    "                           label_id_col='label',\n",
    "                           bbox_col=bbox_col,\n",
    "                           bbox_format=model_parameters.get('bbox_format'),\n",
    "                           transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c56dbac-2ee8-49c4-a0c9-b18cc466082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collates a batch of data samples into a single dictionary for model input.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7fa2cd-6883-47f3-a726-bd47aee69bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoading batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m batch[\u001b[33m'\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#outputs = model(**batch)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "model = dtr.model\n",
    "model.eval()\n",
    "data_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=4, \n",
    "                         collate_fn=collate_fn,\n",
    "                         pin_memory=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b, batch in enumerate(data_loader):\n",
    "        print(f'Loading batch {b + 1}')\n",
    "        batch['pixel_values'].to(device)\n",
    "        batch['labels'].to(device)\n",
    "        #outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ba3778-eb13-4c51-b258-2cb8327b8336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'labels'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f094d0f-aca0-4553-b093-3ee72ae4917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f7bba-d729-41d9-9fe4-5caedd7a3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (4, 4)\n",
    "threshold = 0.02\n",
    "idx = 11\n",
    "file_name = file_name_list[idx]\n",
    "    \n",
    "file = os.path.join(image_dir, file_name)\n",
    "im = ImageData().load_image(file)\n",
    "im = ImageData().np2color(im)\n",
    "df_file = df.loc[df[file_col] == file_name]\n",
    "bbox_list = df_file[bbox_col].tolist()\n",
    "pos_list = df_file[pos_col].tolist()\n",
    "pos_list = sorted([int(pos) for pos in pos_list])\n",
    "    \n",
    "# Predict positions\n",
    "output = dtr.predict(image=im, threshold=threshold)\n",
    "if output is not None:\n",
    "    output_bbox_list = output.get('bboxes')\n",
    "    output_cat_list = output.get('categories')\n",
    "    output_pos_list = [id2label.get(cat) for cat in output_cat_list]\n",
    "    score_list = output.get('scores')\n",
    "    output_label_list = [f'P{p}\\n{s:.2f}' for p, s in zip(output_pos_list, score_list)]\n",
    "\n",
    "    print(f'True: {pos_list}')\n",
    "    print(f'Pred: {sorted(output_pos_list)}')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = plot_boxes(image=im, box_list=bbox_list, label_list=pos_list, color='w', ax=ax)\n",
    "    ax = plot_boxes(image=im, box_list=output_bbox_list, label_list=output_label_list, ax=ax, offset_xy=(0, 200))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb6671-4e8f-477c-9cae-b8e2f2c0e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick one ground truth and one predicted position\n",
    "# So we can calculate the iou\n",
    "true_pos = 12\n",
    "true_bbx = list(bbox_list[pos_list.index(true_pos)])\n",
    "true_bbx = clipxywh(true_bbx, xlim=(0, im.shape[1]), ylim=(0, im.shape[0]), decimals=0)\n",
    "print(true_bbx)\n",
    "\n",
    "pred_pos_idx = output_pos_list.index(true_pos)\n",
    "pred_pos = output_pos_list[pred_pos_idx]\n",
    "print(pred_pos)\n",
    "pred_bbx = output_bbox_list[pred_pos_idx]\n",
    "print(pred_bbx)\n",
    "\n",
    "# Plot this situation\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "ax = plot_boxes(image=im, box_list=[true_bbx], label_list=[true_pos], ax=ax, color='w')\n",
    "ax = plot_boxes(image=im, box_list=[pred_bbx], label_list=[pred_pos], ax=ax, color='r', offset_xy=(0, 20))\n",
    "plt.show()\n",
    "\n",
    "# Calculate the IoU for the predicted bounding box and the ground truth\n",
    "iou = DetectionMetrics.compute_iou(bbox_1=true_bbx, bbox_2=pred_bbx, bbox_format='xywh', method='pt')\n",
    "print(f'IoU for label {true_pos}: {iou}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e006a71-7c31-4dc5-915b-a2c87f1f612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluations\n",
    "score_threshold = 0.02\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Ground truth\n",
    "true_labels = [int(l) for l in df_file[pos_col].tolist()]\n",
    "true_bboxes = df_file[bbox_col].tolist()\n",
    "true_bboxes = [clipxywh(list(box), xlim=x_lim, ylim=y_lim, decimals=0) for box in true_bboxes]\n",
    "print(f'True labels:        {true_labels}')\n",
    "\n",
    "# Predictions\n",
    "output = dtr.predict(image=im, threshold=score_threshold)\n",
    "pred_labels = [int(id2label.get(cat)) for cat in output.get('categories')]\n",
    "pred_bboxes = output.get('bboxes')\n",
    "pred_bboxes = [clipxywh(list(box), xlim=x_lim, ylim=y_lim, decimals=0) for box in pred_bboxes]\n",
    "pred_scores = output.get('scores')\n",
    "print(f'Pred labels:        {pred_labels}')\n",
    "\n",
    "# Determine the false negative samples\n",
    "# Predictions that were missed by the object detection\n",
    "missed = sorted(list(set(true_labels).difference(pred_labels)))\n",
    "print(f'Missed predictions: {missed}')\n",
    "\n",
    "# We want to classify the predictions\n",
    "iou_list = []\n",
    "prediction_list = []\n",
    "for p, p_label in enumerate(pred_labels):\n",
    "    \n",
    "    p_bbox = pred_bboxes[p]\n",
    "    \n",
    "    # When there is no label for this prediction, it is a FP prediction\n",
    "    # NOTE: the FP numbers will be too high if not all of the instances are labeled!\n",
    "    p_prediction = 'FP'\n",
    "    p_iou = np.nan\n",
    "    \n",
    "    # Let's check each ground truth labels if we have a match\n",
    "    pt_iou_list = []\n",
    "    t_bbx_list = []\n",
    "    \n",
    "    for t, t_label in enumerate(true_labels):\n",
    "        if p_label == t_label:\n",
    "            t_bbox = true_bboxes[t]\n",
    "            pt_iou = DetectionMetrics.compute_iou(p_bbox, t_bbox, bbox_format='xywh', method='pt')\n",
    "            pt_iou_list.append(pt_iou)\n",
    "            t_bbx_list.append(t_bbox)\n",
    "            \n",
    "    if len(pt_iou_list) > 0:\n",
    "        p_iou = np.max(pt_iou_list)\n",
    "        t_bbx = t_bbx_list[np.argmax(pt_iou_list)]\n",
    "        if p_iou >= iou_threshold:\n",
    "            p_prediction = 'TP'\n",
    "        \n",
    "        print(f'Prediction for label {p + 1}/{len(pred_labels)}: {p_label}: {p_prediction} with IoU: {p_iou}')\n",
    "        \n",
    "        # Plot the situation when we have a match\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = plot_boxes(image=im, box_list=[t_bbx], label_list=[p_label], ax=ax, color='w')\n",
    "        ax = plot_boxes(image=im, box_list=[p_bbox], label_list=[p_label], ax=ax, color='r', offset_xy=(0, 20))\n",
    "        plt.show()\n",
    "\n",
    "    iou_list.append(p_iou)\n",
    "    prediction_list.append(p_prediction)\n",
    "\n",
    "# What if we predict the same class twice and both are TP?\n",
    "# We count only the first one as a TP, the others we set to FP.\n",
    "c = pd.DataFrame({'pred_label': pred_labels,\n",
    "                  'TP': prediction_list,\n",
    "                  'score': pred_scores,\n",
    "                  'IoU': iou_list})\n",
    "\n",
    "c = c.assign(duplicate_TP=False)\n",
    "\n",
    "# Flip duplicate TP predictions\n",
    "d = c.copy()\n",
    "d.loc[(c.duplicated(subset=['pred_label', 'TP'])) & (c['TP'] == 'TP'), 'TP'] = 'FP'\n",
    "d.loc[(c.duplicated(subset=['pred_label', 'TP'])) & (c['TP'] == 'TP'), 'duplicate_TP'] = True\n",
    "\n",
    "print(pred_labels)\n",
    "print(true_labels)\n",
    "print(iou_list)\n",
    "print(prediction_list)\n",
    "display(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9114d-d3e5-4b1f-9356-26dee574db69",
   "metadata": {},
   "source": [
    "### Prediction classification method ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698332c-dee2-4e0d-b6b6-983e36709537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True labels: {true_labels}')\n",
    "print(f'Pred labels: {pred_labels}')\n",
    "\n",
    "missed, predictions_df = DetectionMetrics.\\\n",
    "    classify_predictions(true_labels=true_labels, \n",
    "                         true_bboxes=true_bboxes, \n",
    "                         pred_labels=pred_labels, \n",
    "                         pred_bboxes=pred_bboxes, \n",
    "                         iou_threshold=iou_threshold)\n",
    "\n",
    "print(missed)\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145438f8-0426-47b5-967f-5cc27029a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the entire data set\n",
    "figsize = (4, 4)\n",
    "score_threshold = 0.02\n",
    "iou_threshold = 0.5\n",
    "\n",
    "idx = 123\n",
    "file_name = file_name_list[idx]\n",
    "file = os.path.join(image_dir, file_name)\n",
    "im = ImageData().load_image(file)\n",
    "im = ImageData().np2color(im)\n",
    "x_lim = (0, im.shape[1])\n",
    "y_lim = (0, im.shape[0])\n",
    "df_file = df.loc[df[file_col] == file_name]\n",
    "bbox_list = [clipxywh(list(box), xlim=x_lim, ylim=y_lim, decimals=0) for box in df_file[bbox_col].tolist()]\n",
    "pos_list = [int(pos) for pos in df_file[pos_col].tolist()]\n",
    "    \n",
    "# Prediction\n",
    "output = dtr.predict(image=im, threshold=score_threshold)\n",
    "if output is not None:\n",
    "    output_bbox_list = output.get('bboxes')\n",
    "    output_cat_list = output.get('categories')\n",
    "    output_pos_list = [id2label.get(cat) for cat in output_cat_list]\n",
    "    score_list = output.get('scores')\n",
    "    output_label_list = [f'P{p}\\n{s:.2f}' for p, s in zip(output_pos_list, score_list)]\n",
    "\n",
    "    print(f'True: {pos_list}')\n",
    "    print(f'Pred: {output_pos_list}')\n",
    "\n",
    "    # Classify predictions\n",
    "    missed, pred_df = DetectionMetrics.classify_predictions(true_labels=pos_list,\n",
    "                                                            true_bboxes=bbox_list,\n",
    "                                                            pred_labels=output_pos_list,\n",
    "                                                            pred_bboxes=output_bbox_list)\n",
    "    print(f'Missed: {missed}')\n",
    "    display(pred_df)\n",
    "\n",
    "    # Calculate precision/recall\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = plot_boxes(image=im, box_list=bbox_list, label_list=pos_list, color='w', ax=ax)\n",
    "    ax = plot_boxes(image=im, box_list=output_bbox_list, label_list=output_label_list, ax=ax, offset_xy=(0, 200))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
